{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first of all i have to say i used some parts of [https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard](http://) for this kernal (including boxcox transformation and tuning hyper parameters in modeling sections.\n",
    "so thank u @serigne!\n",
    "the perpouse of this kernal is not having the most accurate model.i just want to give you a blue print about what you should do for regression problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr()['SalePrice'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets quickly take a peek at some most important attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(train['GrLivArea'],train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outliers:\n",
    "\n",
    "Identifies extreme values in data\n",
    "\n",
    "Outliers are defined as:\n",
    "Values below Q1-1.5(Q3-Q1) or above Q3+1.5(Q3-Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_tukey(x):\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "    iqr = q3-q1   \n",
    "    floor = q1 - 1.5*iqr\n",
    "    ceiling = q3 + 1.5*iqr\n",
    "    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n",
    "    outlier_values = list(x[outlier_indices])\n",
    "\n",
    "    return outlier_indices\n",
    "out=find_outliers_tukey(train['TotalBsmtSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take another look:\n",
    "sns.scatterplot(train['GrLivArea'],train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thats much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot((train['SalePrice']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log transormation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice']=np.log(train['SalePrice'])\n",
    "sns.distplot(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once again much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(objs=[train, test], axis=0,sort=False,ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NaNs and Zeroes:\n",
    "first lets count number of Nans for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### not all 'NANs are missing values\n",
    "#### According to the \"data descriptions\" only NANs for LotFrontage,Electrical,GarageCars,GarageArea,MasVnrType , MasVnrArea & SaleType are Actually missing and nothing has been maped to nan\n",
    "#### we'll deal with them appropiatly \n",
    "\n",
    "first we deal with those 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non meaningfull numericals :\n",
    "we use sklearn SimpleImputer from sklearn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp_num=SimpleImputer(missing_values=np.nan,strategy='mean') #mean for numericals and mode for categoricals\n",
    "dataset[['MasVnrArea','LotFrontage','GarageArea']]=pd.DataFrame(imp_num.fit_transform(dataset[['MasVnrArea','LotFrontage'\n",
    "                                                                                               ,'GarageArea']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non meaningfull Categorical :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cat=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "\n",
    "dataset[['Electrical','MasVnrType','SaleType','MSZoning','Utilities','Exterior1st','Exterior2nd','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType']]=pd.DataFrame(imp_cat.fit_transform(dataset[['Electrical','MasVnrType','SaleType','MSZoning','Utilities','Exterior1st','Exterior2nd','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaningfull NANS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):  \n",
    "    dataset[col] = dataset[col].fillna(0)\n",
    "    \n",
    "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    dataset[col] = dataset[col].fillna('Nothing')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    dataset[col] = dataset[col].fillna(0)\n",
    "    \n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    dataset[col] = dataset[col].fillna('Nothing')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Alley'] = dataset['Alley'].fillna('Nothing')\n",
    "dataset['FireplaceQu'] = dataset['FireplaceQu'].fillna('Nothing')\n",
    "dataset['Fence'] = dataset['Fence'].fillna('Nothing')\n",
    "dataset['PoolQC']=dataset['PoolQC'].fillna('Nothing')\n",
    "dataset['MiscFeature']=dataset['MiscFeature'].fillna('Nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ooookay it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before jumping into dummy variables: the thing about  pd.get_dummies function is it only recognise categorical datas\n",
    "for example:MSZoning variables seems to be numerical but each one is actually represnting a categorical variable.but since it \n",
    "is numerical in our dataset,get_dummies function won't recognize it and will treat them as numerical hence no dummy column will be created\n",
    "so what we are going to do is make sure datas are in right type\n",
    "otherwise there is a lot of ordinal variables that we can use onehot encoding on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'MSSubClass' is categorical variable but it is actually an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['MSSubClass']=dataset['MSSubClass'].astype('str')\n",
    "dataset['MoSold']=dataset['MoSold'].astype('str')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of 2006,2007... label them as 0,1 ... for ease in use\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cat_encoder=LabelEncoder()\n",
    "print(cat_encoder.fit_transform(dataset['YrSold'].values))\n",
    "dataset['YrSold']=cat_encoder.fit_transform(dataset['YrSold'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "external=['ExterQual','ExterCond','HeatingQC']\n",
    "for e in external:\n",
    "    dataset[e]=dataset[e].map({'Ex':4,'Gd':3,'TA':2,'Fa':1,'Po':0})\n",
    "    \n",
    "    \n",
    "basement=['BsmtQual','BsmtCond','GarageQual','GarageCond','FireplaceQu','KitchenQual']\n",
    "for b in basement:\n",
    "    dataset[b]=dataset[b].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'Nothing':0})\n",
    "    \n",
    "\n",
    "dataset['BsmtFinType2']=dataset['BsmtFinType2'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'Nothing':0})\n",
    "dataset['BsmtFinType1']=dataset['BsmtFinType1'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'Nothing':0})\n",
    "dataset['BsmtExposure']=dataset['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'Nothing':0})\n",
    "dataset['LandSlope']=dataset['LandSlope'].map({'Gtl':2,'Mod':1,'Sev':0})\n",
    "dataset['Fence']=dataset['Fence'].map({'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'Nothing':0})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new functions:\n",
    "i'm not comfortable with many features for Year\n",
    "lets create a better feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_UltimateYear_ix (X):\n",
    "    UltimateYear_ix = X[:,YearBuilt]+X[:,YearRemodAdd]+X[:,YrSold]\n",
    "UltimateYear=pd.DataFrame(data={'UltimateYear':dataset['YearBuilt']+dataset['YearRemodAdd']+dataset['YrSold']}))\n",
    "dataset.insert(loc=60,column='UltimateYear',value=UltimateYear)\n",
    "dataset=dataset.drop(['YearBuilt','YearRemodAdd','YrSold'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing some useless variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \n",
    "dataset=dataset.drop(['Utilities'],axis=1)\n",
    "dataset=dataset.drop(['Id'],axis=1) #and drop id column cuase there in no use for it in our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets take a closer look at remainng categorical variables:\n",
    "since we are smart data scientists we dont want to over complicate our model and since this is relativly large feature dataset \n",
    "before to go into dummy variables we are about to see can we get rid of some extra column?so many features can cuase overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in dataset.columns:\n",
    "    if dataset[col_name].dtypes=='object':\n",
    "        unique_cat=len(dataset[col_name].unique())\n",
    "        print(\"feature {col_name} has {unique_cat} unique categories\".format(col_name=col_name,unique_cat=unique_cat)) #intresting syntax!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['Exterior1st'],normalize=True).sort_values(ascending=False)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['Exterior2nd'],normalize=True).sort_values(ascending=False)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['Neighborhood']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['MSSubClass']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dataset['Condition1']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, bucket low frequecy categories as \"Other\"\n",
    "dataset['Exterior1st']=dataset['Exterior1st'].replace(['ImStucc','CBlock','Stone','AsphShn','BrkComm','Stucco','AsbShng','WdShing','BrkFace'],'other')\n",
    "dataset['Exterior2nd']=dataset['Exterior2nd'].replace(['ImStucc','CBlock','Stone','AsphShn','Brk Cmn','Stucco','AsbShng','WdShing','Other','BrkFace'],'other')\n",
    "dataset['Neighborhood']=dataset['Neighborhood'].replace(['Blueste','NPkVill','Veenker','Blmngtn','BrDale','MeadowV','ClearCr'],'other')\n",
    "dataset['MSSubClass']=dataset['MSSubClass'].replace(['150','40','180','45','75'],'other')\n",
    "dataset['Condition1']=dataset['Condition1'].replace(['RRNe','RRNn','PosA','RRAe','PosN','RRAn'],'other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skwed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, skew\n",
    "numeric_feats = dataset.dtypes[dataset.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = dataset[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Cox Transformation of (highly) skewed features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n",
    "\n",
    "Note that setting  λ=0  is equivalent to log1p used above for the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75].dropna()\n",
    "skewness.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75].dropna()\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p,inv_boxcox\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    dataset[feat] = boxcox1p(dataset[feat], lam)\n",
    "    \n",
    "#all_data[skewed_features] = np.log1p(all_data[skewed_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperating our dataset into train and test and prepare for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs_num = len(train)\n",
    "dataset_preprocessed = pd.get_dummies(dataset,drop_first=True)\n",
    "train_preprocessed = dataset_preprocessed[:train_objs_num]\n",
    "test_preprocessed = dataset_preprocessed[train_objs_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just for shuffling the training set before modeling.that is optional i'll leave it to you(just train,we have to maintain order of test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.utils import shuffle\n",
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_preprocessed.drop(['SalePrice'],axis=1)\n",
    "y_train=train_preprocessed['SalePrice']\n",
    "Test=test_preprocessed.drop(['SalePrice'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce dimensionality:\n",
    "as smart data scientists we want to achieve best results with minimum complexity.\n",
    "Such a large set of features can cause overfitting and also slow computing\n",
    "once again we Use feature selection to select the most important features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Such a large set of features can cause overfitting and also slow computing\n",
    "# Use feature selection to select the most important features\n",
    "import sklearn.feature_selection\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=180)\n",
    "selected_features = select.fit(X_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [X_train.columns[i] for i in indices_selected]\n",
    "\n",
    "X_train_selected = X_train[colnames_selected]\n",
    "X_test_selected = Test[colnames_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(colnames_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define a cross validation function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(model):   #cross validation for 5 fold\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train_selected, y_train, scoring=\"neg_mean_squared_error\",cv =5))\n",
    "    return(rmse.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_reg=GradientBoostingRegressor(n_estimators=2000, learning_rate=0.02,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=30, min_samples_split=30, \n",
    "                                   loss='huber')\n",
    "#gb_reg.fit(X_train,y_train);\n",
    "rmsle(gb_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForesst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg=RandomForestRegressor(n_estimators=200,max_features=14)\n",
    "forest_reg.fit(X_train,y_train);\n",
    "rmsle(forest_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "rmsle(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3))\n",
    "rmsle(ENet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='linear', degree=2, coef0=2.5)\n",
    "rmsle(KRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "rmsle(model_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=500,\n",
    "                              max_bin = 90, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "rmsle(model_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianRidge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br=BayesianRidge()\n",
    "rmsle(br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models = (ENet, gb_reg, KRR, lasso,model_lgb))\n",
    "\n",
    "rmsle(averaged_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally for saving your final predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#averaged_models.fit(X_train,y_train)\n",
    "#predictions=(averaged_models.predict(Test))\n",
    "#predictionsdf = pd.DataFrame({'Predictions':np.exp(predictions)})\n",
    "#predictionsdf.to_csv(r'C:\\Users\\Talion\\Desktop\\predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
